Tutorial: The AirBnb Dataset
============================

`SqlSynthGen <https://github.com/alan-turing-institute/sqlsynthgen/>`_, or SSG for short, is a software package that we have written for synthetic data generation, focussed on relational data.
When pointed to an existing relational database, SSG creates another database with the same database schema, and populates it with synthetic data.
By default the synthetic data is crudely low fidelity, but the user is given various ways to configure the behavior of SSG to increase fidelity, while maintaining transparency and control over how the original data is used to inform the synthetic data, to control privacy risks.

In this tutorial, we go through the different mechanisms SSG has for configuring the data generation, and the different levels of fidelity they can provide and different kinds of utility they can have.
To showcase SSG, we will use the `AirBnb User Bookings dataset, available at Kaggle <https://www.kaggle.com/competitions/airbnb-recruiting-new-user-bookings/data>`_.
The original dataset is a collections CSV files, that can be ported to a relational database using `this Python script <https://github.com/alan-turing-institute/sqlsynthgen/blob/migrate-adult-dataset-to-SQL/tests/examples/airbnb/csv_to_database.py>`_ (it requires having SSG `previously installed <https://sqlsynthgen.readthedocs.io/en/latest/installation.html#enduser>`_).
After migration, the database has the following structure:

.. image:: airbnb_db_diagram.png
  :width: 400
  :alt: The AirBnb database diagram.

Default behavior
-----------------

By default, without any user configuration, the data generated by SSG fulfills the schema of the original data.
SSG contains tools for replicating the schema of a source database.
Let us consider that the AirBnb data is contained in the ``airbnb`` database in our local PostgreSQL instance, and we want to port it to the ``dst`` database.
First, we need to provide SSG with the connection parameters, using a ``.env`` file like the following:

**.env**:

.. code-block::

    SRC_HOST_NAME=localhost
    SRC_USER_NAME=postgres
    SRC_USER_NAME=postgres
    SRC_PASSWORD=password
    SRC_DB_NAME=airbnb
    DST_HOST_NAME=localhost
    DST_USER_NAME=postgres
    DST_PASSWORD=password
    DST_DB_NAME=dst

We can start the schema migration process by running the following command::

    $ sqlsynthgen make-tables

When executed successfully, this command makes an ``orm.py`` file, containing the schema of the airbnb database.
To use this file to replicate the schema in ``dst``, we run the following command::

    $ sqlsynthgen create-tables

Furthermore, we can use the ``orm.py`` file to make a Python module that generates synthetic data::

    $ sqlsynthgen make-generators

This file will contain one generator class (not to be confused with Python generator functions) per source database table.

By default, SSG presumes that any primary keys it encounters will be auto-populated when a row is inserted into the table.
This is often true, for example, when a column is declared as the ``SERIAL`` pseudo-type.
However, this is not the case for the AirBnB dataset.
For example, the ``USERS`` table’s primary key ``ID`` column is of type ``VARCHAR``.
Running the next command, ``create-data``, will produce an error::

    $ sqlsynthgen create-data
    ...
    psycopg2.errors.NotNullViolation:

To work around this, we will manually specify how the primary keys should be generated for the ``COUNTRIES`` and ``USERS`` tables by editing the ``ssg.py`` file.
For example, in line 6 we specify that the ``id`` column value should be created using a ``password`` `Mimesis producer <https://mimesis.name/en/master/api.html>`_, which will give us a random string of characters.

**ssg.py**:

.. code-block:: python3

    class usersGenerator:
        num_rows_per_pass = 1

        def __init__(self, src_db_conn, dst_db_conn):
            pass
            self.id = generic.person.password()
            self.date_account_created = generic.datetime.date()
            ...

Mimesis has a wide array of producers for different scenarios.
See `this Python file <https://github.com/alan-turing-institute/sqlsynthgen/blob/migrate-adult-dataset-to-SQL/tests/examples/airbnb/ssg_manual_edit.py>`_ for the full changes.
Now when we run ``create-data``, we get valid, if not very sensible, values in each of our tables. For example:


.. list-table:: age_gender_bkts
   :header-rows: 1

   * - age_bucket
     - country_destination
     - gender
     - population_in_thousands
     - year
   * - 8k$X-en
     - vQjTJ=p*
     - 1m>?l]"}
     - 485
     - 534

SSG’s default generators have minimal fidelity: All data is generated based purely on the datatype of the its column, e.g. random strings in string columns.
Foreign key relations are respected by picking random rows from the table referenced.
Even this synthetic data, nearly the crudest imaginable, can be useful for instance for testing software pipelines.
Note that this data has no privacy implications, since it is only based on the schema.

Vocabulary tables
-----------------

The simplest configuration option available to increase fidelity is to mark some of the tables in the schema to be “vocabulary” tables.
This means that they will be copied verbatim from the original source data into the synthetic data database.
This should of course only be done for tables that hold no privacy-sensitive data, but rather hold fixed non-sensitive lists of concepts or facts that the rest of the schema references.

For instance, in the AirBnB dataset, our relational database has a table for users that has a foreign key reference to a table of world countries. ``users.country_destination`` references the ``countries.country_destination`` column.
Since the ``countries`` table doesn’t contain personal data, we can make it a vocabulary table.
We identify ``countries`` as a vocabulary table in the ``config.yaml`` file:

**config.yaml**

.. code-block:: yaml

    tables:
        countries:
        vocabulary_table: true

The vocabulary tables are exported from the source database when the generator module is made, so we overwrite ``ssg.py`` with one that includes the vocabulary import classes, using the ``--force`` option::

    $ sqlsynthgen make-generators --config-file config.yaml --force

This will export the ``countries`` table rows to a file called ``countries.yaml`` in your current working directory:

.. code-block:: yaml

    - country_destination: AU
      destination_km2: 7741220
      destination_language: eng
      distance_km: 15297.744
      language_levenshtein_distance: 0.0
      lat_destination: -26.853388
      lng_destination: 133.27516
    - country_destination: CA
      destination_km2: 9984670
      destination_language: eng
      distance_km: 2828.1333
      language_levenshtein_distance: 0.0
      lat_destination: 62.393303
      lng_destination: -96.818146
      ...


We need to truncate any tables in our destination database before importing the countries data with::

    $ sqlsynthgen create-vocab

We must now re-edit ``ssg.py`` to add the primary key ``VARCHAR`` workarounds, as we did in section above.
Once this is done, we can generate random data for the other three tables with::

    $ sqlsynthgen create-data

From now on, whenever we make a change to ``config.yaml``, we should re-run these steps:

1. Run ``sqlsynthgen make-generators --config-file config.yaml --force``.
2. Truncate the non-vocabulary database tables.
3. Run ``create-data``.

To recap, “vocabularies” are tables that don’t need synthesising.
By itself this adds only limited utility, since the interesting parts of the data are typically in the non-vocabulary tables, but it saves great amounts of work by fixing some tables with no privacy concerns to have perfect fidelity from the get-go.
Note that one has to be careful in making sure that the tables marked as vocabulary tables truly do not hold privacy sensitive data, otherwise catastrophic privacy leaks are possible, where the original data is exposed raw and in full.
